---
title: "Extract climate information from grids from the DWD Climate Data Center"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Description
The Deutsche Wetterdienst (DWD) offers a large amount of freely accessible climate information on its website ([Climate Data Center](https://www.dwd.de/EN/climate_environment/cdc/cdc_node.html)). 

The climate information can either be accessed on the level of individual climate stations, or in the form of spatial predictions compressed as ASCII grids (*.asc.gz files) on a $1 \times 1$ mÂ² resolution. These grids with climate information are very valuable as they enable to extract climate information for any location in Germany, and thus to obtain site-specific climate information for research sites anywhere in the country. 

As this is a common task within [our department](http://plantecology.uni-goettingen.de), the present project attempts to streamline the extraction, processing and aggregation of climate information for a set of plot coordinates. With simple modifications, the same scripts can also be used to extract site-specific information from any other kind of raster dataset (e.g. [WorldClim data](http://www.worldclim.org/) etc.).

## Getting the DWD raw data
As it would be rather time-consuming to remotely access the DWD datasets from within an R script, the easiest solution is to batch download them all from the FTP server (there are Browser extentions that make this task very easy, such as [DownThemAll](https://addons.mozilla.org/de/firefox/addon/downthemall/) for Mozilla Firefox). _It is also planned to store the datasets in the exchange folder of our department_. 

After downloading the files, batch unpack the *.asc.gz files (can be done by most file archiving software by selecting a list of files, right-clicking and marking "extract here") and delete the original compressed files. 

In the present example, we worked with monthly averages of precipitation. We decided to keep the folder structure of the original dataset (grids for all years separated into folders by months), but it would also be possible (and even easier to handle) to store all grids in the same folder (I decided not to do so because this way I can use the script to show how to deal with stacks of rasters of different types that are stored in different folders).

As it would be impossible to store the complete dataset (almost 5 GB) on GitHub, the folder `/grids` contains samples of each 3 grids for all months as an example to show how to deal with this type of datasets.

## Setting up an R project
To download a local copy of the present project onto your computer, click on the "clone or download" button in the upper right corner of this GitHub page and choose "Download ZIP"."

![Screenshot of the download menu](figures/screenshot_download.png)

When the file is downloaded, unpack it to your desired project directory. 
You can then run `example_script.R` to test if everything works on your system. If you are working with [RStudio](https://www.rstudio.com/), you can open the R project file `DWD_extract.Rproj`, which automatically sets the working directory to the project directory. If you are using a different editor, you will have to do this by hand before running the script.

The following sections will explain step by step what is going on in `example_script.R`, and show you how to modify this script to use it for your own purposes. 

ADD TREE WITH FOLDER STRUCTURE HERE!

## Installation of GDAL and PROJ.4
As this script is based on the `raster` package, which itself relies on the 
[Geospatial Data Abstraction Library (GDAL)](http://www.gdal.org/) and (PROJ.4=[http://proj4.org/], GDAL and PROJ.4 have to be installed before being able to run the script.

In case you are working with Linux, GDAL and PROJ.4 can be installed by opening a shell and entering:

```{r, engine ='bash', eval = FALSE}
sudo apt-get update && sudo apt-get install libgdal-dev libproj-dev

```

Depending on the distribution you are working with, in some cases you might need a newer version of `libgdal-dev` than the version available in the repositories. If you are using Ubuntu and encounter error messages regarding the version of GDAL, you can try to add the [ubuntugis-unstable](https://launchpad.net/~ubuntugis/+archive/ubuntu/ubuntugis-unstable) PPA to your system's repositories (keep in mind that this installs unsupported, experimental packages from an untrusted PPA and might hence be dangerous).

```{r, engine ='bash', eval = FALSE}
sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable
sudo apt-get update
```       

If you are working with Windows, there are several possible ways to install GDAL and PROJ.4, for instance by installing [GISInternals](http://www.gisinternals.com/). Other options are offered on the corresponding websites.

## Working with example_script.R
The next sections describe the different components of `example_script.R`, and show how to run it. Assuming you're working with [RStudio](https://www.rstudio.com/), you can simply open the .Rproj file and `example_script.R`, and then follow the instructions step by step.

### Preparation
First, the packages needed for the analysis have to be loaded. Here's a nice bit of code that checks if all of them are installed, and installs them if they are not available:
```{r preparation1, cache = TRUE, results = 'hide'}
# create list of packages
pkgs <-c("tidyverse", "rgdal", "raster", "gdalUtils")  
# check for existence of packages and install if necessary
to_install<-pkgs[!(pkgs %in% installed.packages()[,1])]
if (length(to_install)>0)  for (i in seq(to_install)) install.packages(to_install[i])
# load all required packages
for (i in pkgs) require(i, character.only = T)

```
The package `tidyverse` is a wrapper around a list of a large amount of very useful packages (e.g. `dplyr`, `purrr`, `ggplot2` and `readr`) that together form a consistent framework for data handling and management. `rgdal` allows R to access the functionalities of the GDAL library, `raster` is a package for efficient raster file handling and `gdalUtils` is used to read CRS strings from .prj files.

### Load plot coordinates 
The dataset with the plot coordinates is stored in `/data/csv`. It can be loaded with
```{r coord1, cache = TRUE, results = 'hide'}
coord <- read_csv("data/csv/Coordinates.csv")
```

Note that I load the dataset with `readr::read_csv()` instead of `utils::read.csv()`. This loads the coordinates in the `tibble` format, which prints more beatifully than a regular `data.frame`:
```{r coord2, cache = TRUE}
coord
```

The function `SpatialPointsDataFrame()` can be used to convert `coord` into an object with explicit spatial information:
```{r coord3, cache = TRUE, results = 'hide'}
coord1 <- SpatialPointsDataFrame(coords = coord[,4:3], data = coord, 
                                 proj4string = CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"))
```

The `proj4string` is a coordinate reference system as provided by the PROJ.4 library (in this case, latitude/longitude as decimal degrees). Note that I specified `coords = coord[,4:3]` - the order of the coordinates had to be reversed because longitude has to come first.


### Loading and handling raster files
To easily work with the contents of the grids folder, it is useful to be able
to work with the `list.files()` function (which lists the contents of a folder).

We can use `list.files()` to get the path to the grid with the precipitation data for the first month of the first year in the interval spanned by the DWD data:
```{r raster1, cache = TRUE}
first <- list.files("grids/precipitation/jan", full.names = TRUE)[1]
first 
```
`full.names = TRUE` assures that the entire path is returned.

Individual raster datasets can be loaded with the `raster()` function. It is easy to load the .asc file corresponding to this path with `raster()`:

```{r raster2, cache = TRUE}
r1 <- raster(first)
r1 
```

Unfortunately, the file is missing the projection information (`coord. ref. : NA`).

The correct coordinate reference system is stored in `grids/projection.prj`
and can be converted to a CRS object using `gdalUtils::gdalsrsinfo()`:

```{r raster3, cache = TRUE}
proj <- gdalsrsinfo("grids/projection.prj", as.CRS = TRUE)
proj
```

The file can be loaded again using the correct CRS information:
```{r raster4, cache = TRUE}
r2 <- raster(first, crs = proj)
r2 # now the coordinate reference system is displayed
```

It is possible to plot the raster information, e.g. to inspect if it was loaded correctly.
```{r raster5, cache = TRUE}
plot(r2)
```

The plot coordinates are in a different coordinate reference system (longitude - latitude). In order to extract information for the site coordinates, they have to be transformed to the same coordinate system, which can be achieved with `spTransform()`

```{r raster6, cache = TRUE}
coord2 <- spTransform(coord1, CRS = proj)
```

In the original dataset, there are hundreds of rasters with climate information for each month. Fortunately, it is not necessary to access them all separately a single raster dataset at a time because it is easy to stack a large list of rasters all at once. Actually it is not even necessary to use a loop to achieve this...

First, list all files in one folder (in this example, precipitation for 
January):
```{r raster7, cache = TRUE}
files <- list.files("grids/precipitation/jan", full.names = TRUE)
files
```

Now all rasters for January can be loaded as a raster stack:
```{r raster8, cache = TRUE}
jan <- stack(files)
jan
```

This is only possible if coordinate system, extent and resolution of all rasters in the folder are equal. The layers in the stack take their names from the .asc objects
in the corresponding folder. In the case of this tutorial, there are only 3 layers because of data storage constraints on GitHub, but if you are working with the original DWD dataset there may be several hundreds.

For raster stacks, for some reason the coordinate reference has to be
set manually after loading:
```{r raster9, cache = TRUE}
projection(jan) <- proj
jan # now the coord. ref. is correct
```


Raster stacks can be easily plotted: 

```{r raster10, cache = TRUE}
plot(jan) # Don't do this when they have lots of layers!
```

The `extract()` function can be used to extract data for specific coordinates from a raster of raster stack. The package `raster` is called explicitly in this case because there's a function called `extract` in the `tidyverse` that may cause confusion.
```{r raster11, cache = TRUE}
extr <- raster::extract(jan, coord2) %>% as.tibble # the output is converted to tibble
```

The output can then be combined with the plot level information in the our tibble called `coord`:

```{r raster12, cache = TRUE}
data <- bind_cols(coord, extr)
data
```
